{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Walmart Sales Forecasting-- Feature Engineering </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import calendar\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading up the dataframes\n",
    "train=pd.read_csv('final_dataframe.csv')\n",
    "test=pd.read_csv('final_dataframe_test.csv')\n",
    "final_test=pd.read_csv('final_future_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reducing Memory by converting all categorical varaibles to integer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here we are also saving these label encoders data so that we can use them to encode for unkown data in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['item_id']=lbl.fit_transform(train['item_id'])\n",
    "test['item_id']=lbl.transform(test['item_id'])\n",
    "final_test['item_id']=lbl.transform(final_test['item_id'])\n",
    "pickle.dump(lbl,open('label_encoder_item_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['dept_id']=lbl.fit_transform(train['dept_id'])\n",
    "test['dept_id']=lbl.transform(test['dept_id'])\n",
    "final_test['dept_id']=lbl.transform(final_test['dept_id'])\n",
    "pickle.dump(lbl,open('label_encoder_dept_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['cat_id']=lbl.fit_transform(train['cat_id'])\n",
    "test['cat_id']=lbl.transform(test['cat_id'])\n",
    "final_test['cat_id']=lbl.transform(final_test['cat_id'])\n",
    "pickle.dump(lbl,open('label_encoder_cat_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['store_id']=lbl.fit_transform(train['store_id'])\n",
    "test['store_id']=lbl.transform(test['store_id'])\n",
    "final_test['store_id']=lbl.transform(final_test['store_id'])\n",
    "pickle.dump(lbl,open('label_encoder_store_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['state_id']=lbl.fit_transform(train['state_id'])\n",
    "test['state_id']=lbl.transform(test['state_id'])\n",
    "final_test['state_id']=lbl.transform(final_test['state_id'])\n",
    "pickle.dump(lbl,open('label_encoder_state_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_name_1']=lbl.fit_transform(train['event_name_1'])\n",
    "test['event_name_1']=lbl.transform(test['event_name_1'])\n",
    "final_test['event_name_1']=lbl.transform(final_test['event_name_1'])\n",
    "pickle.dump(lbl,open('label_encoder_event_name_1.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_name_2']=lbl.fit_transform(train['event_name_2'])\n",
    "test['event_name_2']=lbl.transform(test['event_name_2'])\n",
    "final_test['event_name_2']=lbl.transform(final_test['event_name_2'])\n",
    "pickle.dump(lbl,open('label_encoder_event_name_2.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_type_1']=lbl.fit_transform(train['event_type_1'])\n",
    "test['event_type_1']=lbl.transform(test['event_type_1'])\n",
    "final_test['event_type_1']=lbl.transform(final_test['event_type_1'])\n",
    "pickle.dump(lbl,open('label_encoder_event_type_1.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_type_2']=lbl.fit_transform(train['event_type_2'])\n",
    "test['event_type_2']=lbl.transform(test['event_type_2'])\n",
    "final_test['event_type_2']=lbl.transform(final_test['event_type_2'])\n",
    "pickle.dump(lbl,open('label_encoder_event_type_2.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['year']=lbl.fit_transform(train['year'])\n",
    "test['year']=lbl.transform(test['year'])\n",
    "final_test['year']=lbl.transform(final_test['year'])\n",
    "pickle.dump(lbl,open('label_encoder_year.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Removing Unnecessary columns</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 s, sys: 3.23 s, total: 32.3 s\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Converting snap_CA,snap_WI,snap_TX into one feature named snap\n",
    "train.loc[train['state_id'] == 'CA', 'snap'] = train.loc[train['state_id'] == 'CA']['snap_CA']\n",
    "train.loc[train['state_id'] == 'TX', 'snap'] = train.loc[train['state_id'] == 'TX']['snap_TX']\n",
    "train.loc[train['state_id'] == 'WI', 'snap'] = train.loc[train['state_id'] == 'WI']['snap_WI']\n",
    "train.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "test.loc[test['state_id'] == 'CA', 'snap'] = test.loc[test['state_id'] == 'CA']['snap_CA']\n",
    "test.loc[test['state_id'] == 'TX', 'snap'] = test.loc[test['state_id'] == 'TX']['snap_TX']\n",
    "test.loc[test['state_id'] == 'WI', 'snap'] = test.loc[test['state_id'] == 'WI']['snap_WI']\n",
    "test.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)\n",
    "\n",
    "final_test.loc[final_test['state_id'] == 'CA', 'snap'] = final_test.loc[final_test['state_id'] == 'CA']['snap_CA']\n",
    "final_test.loc[final_test['state_id'] == 'TX', 'snap'] = final_test.loc[final_test['state_id'] == 'TX']['snap_TX']\n",
    "final_test.loc[final_test['state_id'] == 'WI', 'snap'] = final_test.loc[final_test['state_id'] == 'WI']['snap_WI']\n",
    "final_test.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.64 s, sys: 3.6 s, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Weekday as wday are similar features so we remove it\n",
    "#I will make no use of  wm_yr_wk feature\n",
    "train.drop('weekday',axis=1,inplace=True)\n",
    "train.drop('wm_yr_wk',axis=1,inplace=True)\n",
    " \n",
    "test.drop('weekday',axis=1,inplace=True)\n",
    "test.drop('wm_yr_wk',axis=1,inplace=True)\n",
    "\n",
    "final_test.drop('weekday',axis=1,inplace=True)\n",
    "final_test.drop('wm_yr_wk',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.Date Related Features</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.Week Number</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference https://stackoverflow.com/questions/2600775/how-to-get-week-number-in-python/32638267#:~:text=()%5B1%5D%2024-,datetime.,for%20the%20given%20date%20instance.&text=You%20can%20get%20the%20week%20number%20directly%20from%20datetime%20as%20string.\n",
    "def get_week_number(x):\n",
    "    \"\"\"This Function is used to get weeknumber of particular date\"\"\"\n",
    "    date=calendar.datetime.date.fromisoformat(x)\n",
    "    return date.isocalendar()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['week_number']=train['date'].apply(lambda x:get_week_number(x))\n",
    "test['week_number']=test['date'].apply(lambda x:get_week_number(x))\n",
    "final_test['week_number']=final_test['date'].apply(lambda x:get_week_number(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.Season</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.universaltraveller.com.au/destinations/los-angeles/weather\n",
    "def get_season(x):\n",
    "    \"\"\"This function is used to get season in US according to various months\"\"\"\n",
    "    if x in [12,1,2]:\n",
    "        return 0      #\"Winter\"\n",
    "    elif x in [3,4,5]:\n",
    "        return 1   #\"Spring\"\n",
    "    elif x in [6,7,8]:\n",
    "        return 2   #\"Summer\"\n",
    "    else:\n",
    "        return 3   #\"Autumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['season']=train['month'].apply(lambda x:get_season(x))\n",
    "test['season']=test['month'].apply(lambda x:get_season(x))\n",
    "final_test['season']=final_test['month'].apply(lambda x:get_season(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.Quater Start</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quater_begin(x):\n",
    "    \"\"\"This is used to check if day is begining of quater\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==1 and (month in [1,4,7,9])) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quater_start']=train['date'].apply(lambda x:check_if_quater_begin(x))\n",
    "test['quater_start']=test['date'].apply(lambda x:check_if_quater_begin(x))\n",
    "final_test['quater_start']=final_test['date'].apply(lambda x:check_if_quater_begin(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.Quater End</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference https://www.lawinsider.com/dictionary/quarter-end\n",
    "def check_if_quater_end(x):\n",
    "    \"\"\"This is used to check if day is end of quater\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    if (day==31 and month==3) or (day==30 and month==6) or (day==30 and month==9) or (day==31 and month==12):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quater_end']=train['date'].apply(lambda x:check_if_quater_end(x))\n",
    "test['quater_end']=test['date'].apply(lambda x:check_if_quater_end(x))\n",
    "final_test['quater_end']=final_test['date'].apply(lambda x:check_if_quater_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.Month Start</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_start(x):\n",
    "    \"\"\"This is used to check if day is begining of month\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    return 1 if day==1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['month_start']=train['date'].apply(lambda x:month_start(x))\n",
    "test['month_start']=test['date'].apply(lambda x:month_start(x))\n",
    "final_test['month_start']=final_test['date'].apply(lambda x:month_start(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.Month End</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_end(x):\n",
    "    \"\"\"This is used to check if day is end of month\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    year=calendar.datetime.date.fromisoformat(x).year\n",
    "    leap_yr=(year%4==0) # to check if it is a leap year\n",
    "    val=(day==31 and month==1) or (day==29 if leap_yr else day==28) or (day==31 and month==3) or (day==30 and month==4) or\\\n",
    "        (day==31 and month==5) or (day==30 and month==6) or (day==31 and month==7) or (day==31 and month==8) or\\\n",
    "        (day==30 and month==9) or (day==31 and month==10) or (day==30 and month==11) or (day==31 and month==12)\n",
    "    return 1 if val else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['month_end']=train['date'].apply(lambda x:month_end(x))\n",
    "test['month_end']=test['date'].apply(lambda x:month_end(x))\n",
    "final_test['month_end']=final_test['date'].apply(lambda x:month_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7.Year Start</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_start(x):\n",
    "    \"\"\"This is used to check if day is begining of year\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==1 and month==1) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year_start']=train['date'].apply(lambda x:year_start(x))\n",
    "test['year_start']=test['date'].apply(lambda x:year_start(x))\n",
    "final_test['year_start']=final_test['date'].apply(lambda x:year_start(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8.Year End</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_end(x):\n",
    "    \"\"\"This is used to check if day is end of year\"\"\"\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==31 and month==12) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year_end']=train['date'].apply(lambda x:year_end(x))\n",
    "test['year_end']=test['date'].apply(lambda x:year_end(x))\n",
    "final_test['year_end']=final_test['date'].apply(lambda x:year_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Taking Last 28 days from train as CrossValidation so that it can be used while Modeling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation data will be used for hyperparameter tuning\n",
    "cv=train[train['date']>='2016-03-28']\n",
    "train=train[train['date']<'2016-03-28']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Time Series Related Features </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 17.7 s, total: 1min 28s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Firstly we will create these Direct features for train  and CV test and final test data\n",
    "# Code to create one large data for all days\n",
    "gc.collect()\n",
    "tt=pd.concat([train,cv,test,final_test])\n",
    "tt.sort_values(['id','date'],inplace=True)\n",
    "df=tt.pivot_table(index=['item_id','store_id'],columns='date',values='sales')\n",
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.Rolling mean and Rolling Standard Deviation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean\n",
      "Feature created named := roll_14_shift_28_mean\n",
      "Feature created named := roll_30_shift_28_mean\n",
      "Feature created named := roll_60_shift_28_mean\n",
      "Feature created named := roll_360_shift_28_mean\n",
      "Feature created named := roll_7_shift_28_std\n",
      "Feature created named := roll_14_shift_28_std\n",
      "Feature created named := roll_30_shift_28_std\n",
      "Feature created named := roll_60_shift_28_std\n",
      "Feature created named := roll_360_shift_28_std\n",
      "CPU times: user 13min 29s, sys: 3min 2s, total: 16min 31s\n",
      "Wall time: 16min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#this code snippet is used to calculate rolling mean and rolling std of sales data with shift of 28 days \n",
    "# Here we are taking 28 days shift so as to avoid Data Leakage Problem\n",
    "for aggregate in ['mean','std']:\n",
    "    for shif in [28]:\n",
    "        for r in [7,14,30,60,360]:\n",
    "            roll=df.rolling(r,axis=1).agg(aggregate).shift(shif)\n",
    "            dates=roll.columns\n",
    "            name=\"roll_\"+str(r)+\"_shift_\"+str(shif)+\"_\"+aggregate\n",
    "            roll=roll.astype('float16')\n",
    "            roll.reset_index(level=[0,1],inplace=True)\n",
    "            roll=pd.melt(roll,id_vars=['item_id','store_id'],value_vars=dates,var_name='date',value_name=name)\n",
    "            roll.fillna(-1,inplace=True)\n",
    "            train=train.merge(roll,on=['item_id','store_id','date'])\n",
    "            cv=cv.merge(roll,on=['item_id','store_id','date'])\n",
    "            final_test=final_test.merge(roll,on=['item_id','store_id','date'])\n",
    "            test=test.merge(roll,on=['item_id','store_id','date'])\n",
    "            print(\"Feature created named :=\",name)\n",
    "            del roll\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Exponential Weighted Average</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Feature created ewa window of size\n"
     ]
    }
   ],
   "source": [
    "# Adding  Exponential weighted average with shift of 28 days\n",
    "# Shift of 28 days is used to prevent data leakage Problem\n",
    "roll=df.shift(28,axis=1).ewm(alpha=0.99,axis=1,adjust=False).mean()\n",
    "dates=roll.columns\n",
    "roll=roll.astype('float16')\n",
    "roll.reset_index(level=[0,1],inplace=True)\n",
    "roll=pd.melt(roll,id_vars=['item_id','store_id'],value_vars=dates,var_name='date',value_name='direct_ewm')\n",
    "roll.fillna(-1,inplace=True)\n",
    "train=train.merge(roll,on=['item_id','store_id','date'])\n",
    "cv=cv.merge(roll,on=['item_id','store_id','date'])\n",
    "test=test.merge(roll,on=['item_id','store_id','date'])\n",
    "final_test=final_test.merge(roll,on=['item_id','store_id','date'])\n",
    "print(\"Direct Feature created ewa window of size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Lag Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created for lag 28\n",
      "Feature created for lag 35\n",
      "Feature created for lag 42\n",
      "Feature created for lag 49\n",
      "Feature created for lag 56\n",
      "Feature created for lag 63\n",
      "Feature created for lag 70\n",
      "Feature created for lag 77\n",
      "Feature created for lag 84\n",
      "Feature created for lag 91\n",
      "Feature created for lag 98\n",
      "CPU times: user 14min 56s, sys: 3min 22s, total: 18min 18s\n",
      "Wall time: 18min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now we will also calculate lag features with lag of 28,35,42,49,56,63,70,77,84,91,96 days\n",
    "for lag in range(28,100,7):\n",
    "    i='direct_lag_'+str(lag)\n",
    "    lag_i=df.shift(lag,axis=1)\n",
    "    dates=lag_i.columns\n",
    "    lag_i.reset_index(level=[0,1],inplace=True)\n",
    "    lag_i=pd.melt(lag_i,id_vars=['item_id','store_id'],value_vars=dates,var_name='date',value_name=i)\n",
    "    lag_i.fillna(-1,inplace=True)\n",
    "    lag_i[i]=lag_i[i].astype('int16')\n",
    "    train=train.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    cv=cv.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    test=test.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    final_test=final_test.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    print(\"Feature created for lag\",lag)\n",
    "    del lag_i\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Saving All features Created </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.1 s, sys: 696 ms, total: 43.8 s\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.to_csv('train1.csv',index=False)\n",
    "cv.to_csv('cv1.csv',index=False)\n",
    "test.to_csv('test1.csv',index=False)\n",
    "final_test.to_csv('final_test1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Some Points</b>\n",
    "1. Here we have constructed all time series Related features(lag/rolls) with shift of 28 days bcz we may not get stuck into data leakage Problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
